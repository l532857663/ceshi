命名空间:Namespace
curl -vi -X GET \ || curl -vi -X POST \ || curl -vi -X PUT \ || curl -vi -X DELETE \
-H "Accept: text/xml" \

/namespaces
(GET:列出所有名称空间)
"http://example.com:8000/namespaces/"

/namespaces/namespace
(GET:描述特定的命名空间
POST:创建一个新的命名空间
PUT:更改现有命名空间\目前尚未使用
DELETE:删除命名空间\命名空间必须为空)
"http://example.com:8000/namespaces/special_ns"

/namespaces/namespace/tables
(GET:列出特定命名空间中的所有表)
"http://example.com:8000/namespaces/special_ns/tables"

	-------------------------------------------------------------------------------
表:Table
curl -vi -X GET \ || curl -vi -X POST \ || curl -vi -X PUT \ || curl -vi -X DELETE \
-H "Accept: text/xml" \
-H "Content-Type: text/xml" \

/table/schema
(GET:描述指定表的架构
POST:创建新表，或替换现有表的架构
PUT:使用提供的架构片段更新现有表
DELETE:删除表格. 必须使用/table/schemaendpoint，而不仅仅是/table/
)
"http://example.com:8000/users/schema"

POST:
-d '<?xml version="1.0" encoding="UTF-8"?><TableSchema name="users"><ColumnSchema name="cf" /></TableSchema>' \
PUT:
-d '<?xml version="1.0" encoding="UTF-8"?><TableSchema name="users"><ColumnSchema name="cf" KEEP_DELETED_CELLS="true" /></TableSchema>' \

/table/regions
(GET:列出表区域)
"http://example.com:8000/users/regions"

	-------------------------------------------------------------------------------
获取数据操作:Get Operations
curl -vi -X GET \
-H "Accept: text/xml" \
(值为Base-64编码,这需要“Accept”请求标头,其类型可以包含多个列,如xml，json或protobuf)

/table/row (获取单行的所有列.)
"http://example.com:8000/users/row1"

/table/row/column:qualifier
/table/row/column:qualifier/timestamp(获取单个列的值。)
"http://example.com:8000/users/row1/cf:a/1458586888395"

/table/row/column:qualifier/?v=number_of_versions
"http://example.com:8000/users/row1/cf:a?v=2"

	-------------------------------------------------------------------------------
扫描:Scan Operations 
获取Scanner对象,将批处理参数调整为扫描应在批处理中返回的行数。

/table/scanner/
curl -vi -X PUT \
-H "Accept: text/xml" \
-H "Content-Type: text/xml" \
-d '<Scanner batch ="100"> <filter> {"type":"PrefixFilter","value":"u123"} </filter> </Scanner>'
"http://example.com:8000/users/scanner/"

/table/scanner/scanner-id (上边的返回结果)如果扫描仪已耗尽，则返回HTTP状态204。
curl -vi -X GET \
-H "Accept: text/xml" \
"http://example.com:8000/users/scanner/145869072824375522207"

curl -vi -X DELETE \ (删除扫描仪并释放它使用的资源)
-H "Accept: text/xml" \
"http://example.com:8000/users/scanner/145869072824375522207"

	-------------------------------------------------------------------------------
添加操作:Put Operations
在表中写一行. 行,列限定符和值必须均为Base-64编码
/users/fakerow 值是占位符
通过将多行添加到<CellSet>元素来插入多行
还可以将要插入的数据保存到文件中，并使用-d @filename.txt

curl -vi -X PUT \
-H "Accept: text/xml" \
-H "Content-Type: text/xml" \
-d '<?xml version="1.0" encoding="UTF-8" standalone="yes"?><CellSet><Row key="cm93NQo="><Cell column="Y2Y6ZQo=">dmFsdWU1Cg==</Cell></Row></CellSet>' \
"http://example.com:8000/users/fakerow"

-H "Content-Type: text/json" \
-d '{"Row":[{"key":"cm93NQo=", "Cell": [{"column":"Y2Y6ZQo=", "$":"dmFsdWU1Cg=="}]}]}'' \

----------------------------------------------------------------------------------

200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。
201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。
202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）
204 NO CONTENT - [DELETE]：用户删除数据成功。
400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。
401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。
403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。
404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。
406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。
410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。
422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。
500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。

----------------------------------------------------------------------------------
schema:
----element: 
	----complexType: <Version>

	----complexType: <TableList>

	----complexType: <TableInfo>

	----complexType: <TableSchema>

	----complexType: <CellSet> type="tns:CellSet"
		----sequence\element:
			<row> 	type="tns:Row"
	----complexType: <Row> type="tns:Row"
		----sequence\element:
			(key) 	type="base64Binary"
			<cell> 	type="tns:Cell"

	----complexType: <Cell> type="tns:Cell"
		----sequence\element:
			----simpleType: value base="base64Binary"
		----attribute:
			(column) type="base64Binary"
			(timestamp) type="int"

	----complexType: <Scanner> type="tns:Scanner"
		----sequence\element:
			column	type="base64Binary"
			<filter>	type="string"
		----attribute:
			(startRow)	type="base64Binary"
			(endRow)	type="base64Binary"
			(batch)		type="int"
			(startTime)	type="int"
			(endTime)	type="int"
//eg: 网上例子
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<CellSet>
	<Row key="cm93NQo=">
		<Cell column="Y2Y6ZQo=">
			dmFsdWU1Cg==
		</Cell>
	</Row>
</Cellset>

<Scanner startRow="cm93MDE=" endRow="cm93MDg=" batch="100">
	<filter>
	{
		"type":"SingleColumnValueFilter",
		"latestVersion":true,
		"ifMissing":true, 
		"qualifier":"Y29sMQ==",
		"family":"ZmFtaWx5",
		"op":"EQUAL",
		"comparator":{"value":"MQ==","type":"BinaryComparator"}
	}
	</filter>
</Scanner>

过滤器
Filter:
RowFilter(筛选出匹配的所有的行)
PrefixFilter(筛选出具有特定前缀的行键的数据)
ColumnPrefixFilter(按照列名的前缀来筛选单元格的)
ValueFilter(按照具体的值来筛选单元格的过滤器)
KeyOnlyFilter(这个过滤器唯一的功能就是只返回每行的行键，值全部为空)

RandomRowFilter(本过滤器的作用就是按照一定的几率（<=0会过滤掉所有的行，>=1会包含所有的行）来返回随机的结果集，对于同样的数据集，多次使用同一个RandomRowFilter会返回不通的结果集)
FirstKeyOnlyFilter(返回的结果集中只包含第一列的数据,它在找到每行的第一列之后会停止扫描)
ColumnCountGetFilter(这个过滤器来返回每行最多返回多少列，并在遇到一行的列数超过我们所设置的限制值的时候，结束扫描操作)
SingleColumnValueFilter(用一列的值决定这一行的数据是否被过滤)

比较运算符 CompareFilter.CompareOp
EQUAL            相等
GREATER          大于
GREATER_OR_EQUAL 大于等于
LESS             小于
LESS_OR_EQUAL    小于等于
NOT_EQUAL        不等于

BinaryComparator        匹配完整字节数组 //二进制  转base64使用
BinaryPrefixComparator  匹配字节数组前缀 
BitComparator
NullComparator
RegexStringComparator   正则表达式匹配
SubstringComparator     子串匹配

//根据row_key前缀扫描数据,结果是匹配数据的所有行
scan "users_test",FILTER=>"PrefixFilter ('users_qwe')"
<Scanner batch="3" startRow="" endRow="">
<filter>{"type":"PrefixFilter","value":"dXNlcnNfcXdl"}</filter>
</Scanner>

//根据某字段值来扫描数据,结果只有指定值的对应字段
scan "users_test",FILTER=>"(ValueFilter(=,'binary:admin')"
<Scanner batch="3" startRow="" endRow="">
<filter>{"type":"ValueFilter","op":"EQUAL","comparator":{"type":"BinaryComparator","value":"YWRtaW4="}}</filter>
</Scanner>

//根据某字段值的一部分来扫描数据 *值不能转base64
scan "users_test",FILTER=>"ValueFilter(=,'substring:888')"
<Scanner batch="3" startRow="" endRow="">
<filter>{"type":"ValueFilter","op":"EQUAL","comparator":{"type":"SubstringComparator","value":"888"}}</filter>
</Scanner>

Stargate Scanner Filter Examples
=================================

## Introduction

So yeah... no documentation for the HBase REST API in regards to what should a filter look like...

So I installed Eclipse, got the library, and took some time to find some of the (seemingly) most useful filters you could use. I'm very green at anything regarding HBase, and I hope this will help anyone trying to get started with it.

What I discovered is that basically, attributes of the filter object follow the same naming than in the documentation. For this reason, I have made the link clickable and direct them to the HBase Class documentation attached to it; check for the instantiation argument names, and you will have your attribute list (more or less).

Don't forget, values are encoded.

## References:

* [HBase REST Filter (SingleColumnValueFilter)](http://stackoverflow.com/questions/9302097/hbase-rest-filter-singlecolumnvaluefilter)
* [HBase Intra-row scanning](http://stackoverflow.com/questions/13119369/hbase-intra-row-scanning)
* [HBase Book / Chapter on Client Filter](http://hbase.apache.org/book/client.filter.html)

### [ColumnPrefixFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.html)
```
{
  "type": "ColumnPrefixFilter",
  "value": "cHJlZml4"
}
```

### [ColumnRangeFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnRangeFilter.html)
```
{
  "type": "ColumnRangeFilter",
  "minColumn": "Zmx1ZmZ5",
  "minColumnInclusive": true,
  "maxColumn": "Zmx1ZmZ6",
  "maxColumnInclusive": false
}
```

### [ColumnPaginationFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.html)

Could not generate an example, but I guess it should be pretty simple to test if it works just by intuitively plugging variables a certain way...

### [DependentColumnFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/DependentColumnFilter.html)

null

### [FamilyFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FamilyFilter.html)
```
{
  "type": "FamilyFilter",
  "op": "EQUAL",
  "comparator": {
    "type": "BinaryComparator",
    "value": "dGVzdHJvdw\u003d\u003d"
  }
}
```

### [FilterList with RowFilter and ColumnRangeFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FilterList.html)
```
{
  "type": "FilterList",
  "op": "MUST_PASS_ALL",
  "filters": [
    {
      "type": "RowFilter",
      "op": "EQUAL",
      "comparator": {
        "type": "BinaryComparator",
        "value": "dGVzdHJvdw\u003d\u003d"
      }
    },
    {
      "type": "ColumnRangeFilter",
      "minColumn": "Zmx1ZmZ5",
      "minColumnInclusive": true,
      "maxColumn": "Zmx1ZmZ6",
      "maxColumnInclusive": false
    }
  ]
}
```

### [FirstKeyOnlyFilter (Can be used for more efficiently perform row count operation)](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html)
```
{
  "type": "FirstKeyOnlyFilter"
}
```

### [InclusiveStopFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/InclusiveStopFilter.html)
```
{
  "type": "InclusiveStopFilter",
  "value": "cm93a2V5"
}
```

### [MultipleColumnPrefixFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.html)
```
{
  "type": "MultipleColumnPrefixFilter",
  "prefixes": [
    "YWxwaGE\u003d",
    "YnJhdm8\u003d",
    "Y2hhcmxpZQ\u003d\u003d"
  ]
}
```

### [PageFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/PageFilter.html)
```
{
  "type": "PageFilter",
  "value": "10"
}
```

### [PrefixFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/PrefixFilter.html)
```
{
  "type": "PrefixFilter",
  "value": "cm93cHJlZml4"
}
```

### [QualifierFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/QualifierFilter.html)
```
{
  "type": "QualifierFilter",
  "op": "GREATER",
  "comparator": {
    "type": "BinaryComparator",
    "value": "cXVhbGlmaWVycHJlZml4"
  }
}
```

### [RowFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/RowFilter.html)
```
{
  "type": "RowFilter",
  "op": "EQUAL",
  "comparator": {
    "type": "BinaryComparator",
    "value": "dGVzdHJvdw\u003d\u003d"
  }
}
```

### [SingleColumnValueFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.html)
```
{
  "type": "SingleColumnValueFilter",
  "op": "EQUAL",
  "family": "ZmFtaWx5",
  "qualifier": "Y29sMQ\u003d\u003d",
  "latestVersion": true,
  "comparator": {
    "type": "BinaryComparator",
    "value": "MQ\u003d\u003d"
  }
}
```

### [TimestampsFilter](http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/TimestampsFilter.html)
```
{
  "type": "TimestampsFilter",
  "timestamps": [
    "1351586939"
  ]
}
```
